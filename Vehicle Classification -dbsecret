databricks-logo Vehicle Classification -dbsecret(Python) Import Notebook
Mount Directory


Out[2]: True
%fs ls /mnt/
dbfs:/mnt/images/	images/	0
dbfs:/mnt/raw/	raw/	0
dbfs:/mnt/raw2/	raw2/	0
path	name	size
 
dbutils.fs.mounts()
Out[1]: 
[MountInfo(mountPoint='/mnt/images', source='wasbs://raw@ehsdatalakepoc.blob.core.windows.net', encryptionType=''),
 MountInfo(mountPoint='/mnt/raw2', source='wasbs://raw@ehsdatalakepoc.blob.core.windows.net', encryptionType=''),
 MountInfo(mountPoint='/mnt/raw', source='wasbs://raw@ehsdatalakepoc.blob.core.windows.net', encryptionType=''),
 MountInfo(mountPoint='/', source='DatabricksRoot', encryptionType=''),
 MountInfo(mountPoint='/databricks-datasets', source='databricks-datasets', encryptionType=''),
 MountInfo(mountPoint='/databricks-results', source='databricks-results', encryptionType='')]
#mount test images from data lake
#uses parameters from 
#shared_access_key is the blob connection url

shaded.databricks.org.apache.hadoop.fs.azure.AzureException: shaded.databricks.org.apache.hadoop.fs.azure.AzureException: Unable to access container raw in account ehsdatalakepoc.blob.core.windows.net using anonymous credentials, and no credentials found for them  in the configuration.
dbutils.fs.mounts()
Out[10]: 
[MountInfo(mountPoint='/', source='DatabricksRoot', encryptionType=''),
 MountInfo(mountPoint='/databricks-datasets', source='databricks-datasets', encryptionType=''),
 MountInfo(mountPoint='/databricks-results', source='databricks-results', encryptionType='')]
%fs ls dbfs:/mnt/images/Vehicle-Detection/
java.io.FileNotFoundException: File/7446494931291187/mnt/images/Vehicle-Detection does not exist.
Keras Deep Learning Example
#import libraries
import numpy as np

import matplotlib.pyplot as plt
#%matplotlib inline
import os
from keras.models import Sequential
from keras.layers.core import Dense, Activation, Flatten, Dropout
from keras.layers.convolutional import Convolution2D
from keras.layers.pooling import MaxPooling2D
import pandas as pd
import datetime
import matplotlib.image as mpimg
Using TensorFlow backend.
#create scaling image function
from sklearn.utils import shuffle

def scaling(img,scale_x,scale_y):
    """
    Input an image in a numpy array, scale of the x and ydirection as a decimal
    Outputs a numpy array as the scaled image
    """
    return cv2.resize(img,None,fx=scale_x, fy=scale_y, interpolation = cv2.INTER_LINEAR)
  

#create normalizing function to normailize augmented data set
def min_max_normalization(x,min,max):
    """
    This function takes an n by m array and normalizes each value based on the average of min and max values
    of the RBG scale (min=0 and max=255).
    
    It return an n by m array
    """
    avg_value=(max+min)/2.0
    norm_array = np.zeros(x.shape)+avg_value
    normalized_x= (x-norm_array)/norm_array
    return normalized_x

  
def generator(samples, batch_size=32):
    num_samples = len(samples)
    #print('generator initiated')
    #idx=0
    while 1: # Loop forever so the generator never terminates
        shuffle(samples)
        for offset in range(0, num_samples, batch_size):
            batch_samples = samples[offset:offset+batch_size]

            images = []
            angles = []
            for batch_sample in batch_samples:
                name = batch_sample[2]
                center_image = load_image_from_uri(name)[0]
                center_angle = batch_sample[1]
                images.append(center_image)
                angles.append(center_angle)
                #images.append(np.fliplr(center_image))
                #angles.append([1-center_angle,center_angle])

            X_train = np.array(images)
            y_train = np.array(angles)
            yield shuffle(X_train, y_train)
            #print('generator yielded a batch %d' % idx)
            #idx += 1
  
model = Sequential()
# TODO: Re-construct the network and add a convolutional layer before the flatten layer.
# Create the Sequential model
#convolutuonal layer with 10 layers and 3 by 3, 2x2 max pooling and activation
model.add(Convolution2D(5, (3, 3), padding='valid', input_shape=(64, 64, 3)))
model.add(MaxPooling2D((2,2)))
model.add(Activation('relu'))

#convolutuonal layer with 20 layers and 3 by 3, 2x2 max pooling and activation
#model.add(Convolution2D(8, 3, 3, border_mode='valid'))
#model.add(MaxPooling2D((2,2)))
#model.add(Activation('relu'))

#convolutuonal layer with 20 layers and 3 by 3, 2x2 max pooling and activation
#model.add(Convolution2D(30, 3, 3, border_mode='valid'))
#model.add(MaxPooling2D((2,2)))
#model.add(Activation('relu'))

#a flatten layer
model.add(Flatten())

#1st layer dense to 800, activation and 50% dropout
#model.add(Dense(400))
#model.add(Activation('relu'))
#model.add(Dropout(0.5))

#2nd layer dense to 500, activation and 50% dropout
model.add(Dense(200))
model.add(Activation('relu'))
model.add(Dropout(0.5))

#3rd layer dense to 200, activation and 50% dropout
model.add(Dense(100))
model.add(Activation('relu'))
model.add(Dropout(0.5))

#4th layer dense to 100, activation and 50% dropout
model.add(Dense(80))
model.add(Activation('relu'))
model.add(Dropout(0.5))

#5th layer dense to 64, activation and 50% dropout
model.add(Dense(64))
model.add(Activation('relu'))
model.add(Dropout(0.5))

#6th layer dense to 32 and activation
model.add(Dense(32))
model.add(Activation('relu'))

#7th layer dense to 16 and activation
model.add(Dense(16))
model.add(Activation('relu'))

#8th layer dense to 1 for regression output
model.add(Dense(2))

# 5th Layer - Add a ReLU activation layer
model.add(Activation('softmax'))

model.save('/tmp/model-small.h5')  # saves to the local filesystem
dbfs_model_small_path = 'dbfs:/tmp/model-small.h5'
#dbutils.fs.cp('file:/tmp/model-small.h5', dbfs_model_small_path)
import PIL.Image
import numpy as np
from keras.applications.imagenet_utils import preprocess_input

def load_image_from_uri(local_uri):
  img = (PIL.Image.open(local_uri).convert('RGB').resize((64, 64), PIL.Image.ANTIALIAS))
  img_arr = min_max_normalization(np.array(img).astype(np.float32), 0.0, 255.0)
  img_tnsr = img_arr[np.newaxis, :]
  return img_tnsr
from pyspark.ml.image import ImageSchema
from pyspark.sql.functions import lit
from sparkdl.image import imageIO
import pyspark.ml.linalg as spla
import pyspark.sql.types as sptyp
import numpy as np

def CreateTrainImageUriandLabels(image_uris, label, cardinality):
  # Create image categorical labels (integer IDs)
  local_rows = []
  for uri in image_uris:
    label_inds = np.zeros(cardinality)
    label_inds[label] = 1.0
    one_hot_vec = spla.Vectors.dense(label_inds.tolist())
    _row_struct = {"uri": uri, "one_hot_label": one_hot_vec, "label": float(label)}
    row = sptyp.Row(**_row_struct)
    local_rows.append(row)

  image_uri_df = sqlContext.createDataFrame(local_rows)
  return image_uri_df

weights = [0.50, 0.25, 0.25]
seed = 42  
label_cardinality = 2
car_path ="/mnt/images/Vehicle-Detection/vehicles/"
noncar_path ="/mnt/images/Vehicle-Detection/non-vehicles/"

car_files = ["/dbfs" + str(k.path)[5:] for f in dbutils.fs.ls(car_path) for k in dbutils.fs.ls(str(f.path)[5:]) if str(f.path).split("/")[-1]!=".DS_Store" and str(k.path).split("/")[-1]!=".DS_Store"]  # make "local" file paths for images
car_uri_df = CreateTrainImageUriandLabels(car_files,1,label_cardinality)
noncar_files = ["/dbfs" + str(k.path)[5:] for f in dbutils.fs.ls(noncar_path) for k in dbutils.fs.ls(str(f.path)[5:]) if str(f.path).split("/")[-1]!=".DS_Store" and str(k.path).split("/")[-1]!=".DS_Store"]  # make "local" file paths for images
noncar_uri_df = CreateTrainImageUriandLabels(noncar_files,0,label_cardinality)

noncar_train, noncar_test, noncar_val = noncar_uri_df.randomSplit(weights,seed)  # use larger training sets (e.g. [0.6, 0.4] for non-community edition clusters)
car_train, car_test, car_val = car_uri_df.randomSplit(weights,seed)     # use larger training sets (e.g. [0.6, 0.4] for non-community edition clusters)
train_df_uri = noncar_train.unionAll(car_train)
test_df_uri = noncar_test.unionAll(car_test)
val_df_uri = noncar_val.unionAll(car_val)
# Under the hood, each of the partitions is fully loaded in memory, which may be expensive.
# This ensure that each of the paritions has a small size.
#train_df_uri = train_df_uri.repartition(100)
#test_df_uri = test_df_uri.repartition(100)
train_df_uri.cache()
test_df_uri.cache()
val_df_uri.cache()
#["/dbfs" + str(k.path)[5:] for f in dbutils.fs.ls(image_path) for k in dbutils.fs.ls(str(f.path)[5:]) if str(f.path).split("/")[-1]!=".DS_Store" and str(k.path).split("/")[-1]!=".DS_Store"]
Out[6]: DataFrame[label: double, one_hot_label: vector, uri: string]
train_generator = generator(train_df_uri.collect(), batch_size=20)
test_generator = generator(test_df_uri.collect(), batch_size=20)
val_generator = generator(val_df_uri.collect(), batch_size=20)
import math
val_steps = math.ceil(len(val_df_uri.collect())/20)

train_steps = math.ceil(len(train_df_uri.collect())/20)
test_steps = math.ceil(len(test_df_uri.collect())/20)

model.compile('Adam', 'categorical_crossentropy', ['accuracy'])
#history = model.fit(data, labels, batch_size=16, nb_epoch=5, validation_split=0.2)
model.fit_generator(train_generator, samples_per_epoch= train_steps, validation_data=val_generator, validation_steps=val_steps, epochs=1, use_multiprocessing=True, workers=4)
/local_disk0/tmp/1549573088900-0/PythonShell.py:4: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=<generator..., validation_steps=222, epochs=1, use_multiprocessing=True, workers=4, steps_per_epoch=440)`
  import errno
/databricks/python/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.
  UserWarning('Using a generator with `use_multiprocessing=True`'
Epoch 1/1

  1/440 [..............................] - ETA: 3:53:43 - loss: 0.6400 - acc: 0.7500
  3/440 [..............................] - ETA: 1:17:46 - loss: 0.4981 - acc: 0.8833
  5/440 [..............................] - ETA: 54:00 - loss: 0.3799 - acc: 0.9200  
  6/440 [..............................] - ETA: 45:01 - loss: 0.3430 - acc: 0.9250
  7/440 [..............................] - ETA: 44:10 - loss: 0.3010 - acc: 0.9357
  8/440 [..............................] - ETA: 39:04 - loss: 0.2649 - acc: 0.9437
  9/440 [..............................] - ETA: 37:59 - loss: 0.2409 - acc: 0.9500
 10/440 [..............................] - ETA: 34:33 - loss: 0.2215 - acc: 0.9550
Cancelled
model.evaluate_generator(test_generator, steps=test_steps, workers=3, use_multiprocessing=True, verbose=0)
model.save('/tmp/model-small.h5')  # saves to the local filesystem

model.evaluate_generator(test_generator, steps=test_steps, workers=3, use_multiprocessing=True)
from sparkdl.estimators.keras_image_file_estimator import KerasImageFileEstimator

dbutils.fs.cp(dbfs_model_small_path, 'file:/tmp/model-small-tmp.h5')
estimator = KerasImageFileEstimator(inputCol="uri",
                                    outputCol="prediction",
                                    labelCol="one_hot_label",
                                    imageLoader=load_image_from_uri,
                                    kerasOptimizer='adam',
                                    kerasLoss='categorical_crossentropy',
                                    modelFile='/tmp/model-small-tmp.h5')
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

paramGrid = (
  ParamGridBuilder()
  .addGrid(estimator.kerasFitParams, [{"batch_size": 16, "verbose": 0},
                                      {"batch_size": 16, "verbose": 0},
                                      {"batch_size": 16, "verbose": 0},
                                      {"batch_size": 16, "verbose": 0},
                                      {"batch_size": 16, "verbose": 0}])
  .build()
)
mc = BinaryClassificationEvaluator(rawPredictionCol="prediction", labelCol="label" )
cv = CrossValidator(estimator=estimator, estimatorParamMaps=paramGrid, evaluator=mc, numFolds=2)

cvModel = cv.fit(train_df_uri)
mc.evaluate(cvModel.transform(test_df_uri))
Deep Image Featurizer
from pyspark.ml.image import ImageSchema
from sparkdl.image import imageIO
from pyspark.sql.functions import lit
import os
cars_dir = os.listdir("/dbfs/mnt/images/Vehicle-Detection/vehicles/")
noncars_dir = os.listdir("/dbfs/mnt/images/Vehicle-Detection/non-vehicles/")
files = []
i = 0
for folder in cars_dir:
  if folder != ".DS_Store":
    if i==0:
      car_df = ImageSchema.readImages("/mnt/images/Vehicle-Detection/vehicles/"+folder).withColumn("label", lit(1))
    else:
      car_df1 = ImageSchema.readImages("/mnt/images/Vehicle-Detection/vehicles/"+folder).withColumn("label", lit(1))
      car_df = car_df.unionAll(car_df1)
    i=i+1
i=0
for folder in noncars_dir:
  if folder != ".DS_Store":
    if i==0:
      noncar_df = ImageSchema.readImages("/mnt/images/Vehicle-Detection/non-vehicles/"+folder).withColumn("label", lit(0))
    else:
      noncar_df1 = ImageSchema.readImages("/mnt/images/Vehicle-Detection/non-vehicles/"+folder).withColumn("label", lit(0))
      noncar_df = noncar_df.unionAll(noncar_df1)
    i=i+1
car_df.show()
car_df.count()
car_df = ImageSchema.readImages("/mnt/images/Vehicle-Detection/vehicles/").withColumn("label", lit(1))
car_df.count()
weights = [0.90, 0.10, 0.0]
seed = 42
car_train, car_test, _ = car_df.randomSplit(weights,seed)  # use larger training sets (e.g. [0.6, 0.4] for non-community edition clusters)
noncar_train, noncar_test, _ = noncar_df.randomSplit(weights,seed) 

train_df = car_train.unionAll(noncar_train)
test_df = car_test.unionAll(noncar_test)

train_df = train_df.repartition(100)
test_df = test_df.repartition(100)
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
from sparkdl import DeepImageFeaturizer 

featurizer = DeepImageFeaturizer(inputCol="image", outputCol="features", modelName="InceptionV3")
lr = LogisticRegression(maxIter=20, regParam=0.05, elasticNetParam=0.3, labelCol="label")
p = Pipeline(stages=[featurizer, lr])

p_model = p.fit(train_df)
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

tested_df = p_model.transform(test_df)
evaluator = MulticlassClassificationEvaluator(metricName="accuracy")
print("Test set accuracy = " + str(evaluator.evaluate(tested_df.select("prediction", "label"))))
display(tested_df.filter(tested_df['label']!=tested_df['prediction']).select("image", "label", "prediction"))
images = sc.binaryFiles("/dbfs/mnt/images/Vehicle-Detection/vehicles/GTI_Far/")
image_to_array = lambda rawdata: np.asarray(Image.open(StringIO(rawdata)))
images.values().map(image_to_array)

